{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression (Generalized linear regression models)\n",
    "\n",
    "## ðŸ’¡Idea \n",
    "\n",
    "**Use a linear model to fit nonlinear data**: \n",
    "add powers of each feature as new features, then train a linear model on this extended set of features.\n",
    "\n",
    "## Generalize Linear Regression to Polynomial Regression\n",
    "\n",
    "In [Linear Regression](quiver-note-url/EBA4D283-8715-4B58-AB63-8B343A22E36C), $f$ is modelled as linear in $\\boldsymbol{x}$ and $\\boldsymbol{w}$\n",
    "\n",
    "$\n",
    "f(x) = \\hat{\\boldsymbol{x}}^T \\boldsymbol{w}\n",
    "$\n",
    "\n",
    "Rewrite it more generally:\n",
    "\n",
    "$\n",
    "f(x) = \\phi(\\boldsymbol{x})^T \\boldsymbol{w}\n",
    "$\n",
    "\n",
    "- $\\phi(\\boldsymbol{x})$: vector valued funtion of the input vector $\\boldsymbol{x}$ (also called \"**linear basis function models**\")\n",
    "  - $\\phi_i(\\boldsymbol{x})$: **basis functions**\n",
    "\n",
    "In principle, this allows us to **learn any non-linear function**, if we know suitable basis functions (which is typically not the case ðŸ¤ª).\n",
    "\n",
    "### Example 1:\n",
    "\n",
    "$\\boldsymbol{x}=\\left[\\begin{array}{c}{x_1 \\\\ x_2}\\end{array}\\right] \\in \\mathbb{R}^{2}$\n",
    "\n",
    "$\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^3, \\left[\\begin{array}{c}{x_1 \\\\ x_2}\\end{array}\\right] \\mapsto \\left[\\begin{array}{c}{1 \\\\ x_1 \\\\ x_2}\\end{array}\\right]\\qquad $(I.e.: $\\phi_1(\\boldsymbol{x}) = 1, \\phi_2(\\boldsymbol{x}) = x_1, \\phi_3(\\boldsymbol{x}) = x_2$)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Pol_Reg_Example_2.png\" alt=\"Pol_Reg_Example_2\" style=\"zoom:50%;\" />\n",
    "\n",
    "### Example 2\n",
    "\n",
    "$\\boldsymbol{x}=\\left[\\begin{array}{c}{x_1 \\\\ x_2}\\end{array}\\right] \\in \\mathbb{R}^{2}$\n",
    "\n",
    "$\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^5, \\left[\\begin{array}{c}{x_1 \\\\ x_2}\\end{array}\\right] \\mapsto \\left[\\begin{array}{c}{1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2}\\end{array}\\right]$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Pol_Reg_Example_3.png\" alt=\"Pol_Reg_Example_3\" style=\"zoom:50%;\" />\n",
    "\n",
    "### Optimal value of $\\boldsymbol{w}$\n",
    "\n",
    "$\n",
    "\\boldsymbol{w}^{*}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}, \\qquad \\mathbf{\\Phi}=\\left[\\begin{array}{c}{\\phi_{1}^{T}} \\\\ {\\vdots} \\\\ {\\phi_{n}^{T}}\\end{array}\\right]\n",
    "$\n",
    "\n",
    "(The same as in Linear Regression, just the data matrix is now replaced by the basis function matrix)\n",
    "\n",
    "## Challenge of Polynomial Regression: Overfitting\n",
    "\n",
    "<img src=\"https://i0.wp.com/csmoon-ml.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-19-at-11.06.04-AM.png?fit=640%2C213\" style=\"zoom:100%; background-color:white\">\n",
    "\n",
    "Reason: \n",
    "**Too complex model** (Degree of the polynom is too high!). It fits the noise and has unspecified behaviour between the training points.ðŸ˜­\n",
    "\n",
    "Solution: Regularization\n",
    "\n",
    "## Regularization\n",
    "\n",
    "_Regularization: Constrain a model to make it simpler and reduce the task of overfitting._\n",
    "\n",
    "ðŸ’¡ **Avoid overfitting by forcing the weights $\\boldsymbol{w}$ to be small**\n",
    "\n",
    "> Assume that our model has degree of 3 ($x^1, x^2, x^3$), and the corresponding parameters/weights are $w_1, w_2, w_3$. If we force $w_3=0$, then $w_3 x^3 = 0$, meaning that the model now has only degree of 2. In other words. the model is somehow simpler.\n",
    "\n",
    "In general, a regularized model has the following cost/objective function:\n",
    "\n",
    "$\n",
    "\\underbrace{E_D(\\boldsymbol{w})}_{\\text{Data term}} + \\underbrace{\\lambda E_W(\\boldsymbol{w})}_{\\text{Regularization term}}\n",
    "$\n",
    "\n",
    "$\\lambda$: regularization factor (hyperparameter, need to be tuned manually), controls how much you want to regularize the model.\n",
    "\n",
    "\n",
    "### Regularized Least Squares (Ridge regression)\n",
    "\n",
    "Consists of:\n",
    "\n",
    "- **Sum of Squareds Error (SSE)** function\n",
    "- quadratic regulariser ($L_2$-Regularization)\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "L_{\\text {ridge }} \n",
    "&= \\mathbf{SSE} + \\lambda \\|w\\|^2 \\\\ \n",
    "&= (\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})^{T}(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})+\\lambda \\boldsymbol{w}^{T} \\boldsymbol{w}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Solution: \n",
    "\n",
    "$\\boldsymbol{w}_{\\mathrm{ridge}}^{*}=\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}$\n",
    "\n",
    "- $\\boldsymbol{I}$: Identity matrix\n",
    "- $\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\boldsymbol{I}\\right)$ is full rank and can be easily inverted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
