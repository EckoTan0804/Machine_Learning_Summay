{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernelized Ridge Regression\n",
    "\n",
    "## Kernel regression\n",
    "\n",
    "### Kernel identities\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi}_{X}=\\left[\\begin{array}{c}\n",
    "                                            \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{1}\\right)^{T} \\\\\n",
    "                                            \\vdots \\\\\n",
    "                                            \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{N}\\right)^{T}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{N \\times d} , \\qquad \\left( \\boldsymbol{\\Phi}_{X}^T = \\left[ \\boldsymbol{\\phi}(x_1), \\dots, \\boldsymbol{\\phi}(x_N)\\right] \\in \\mathbb{R}^{d \\times N} \\right)\n",
    "\\vdots \\\\\n",
    "$$\n",
    "\n",
    "then the following identities hold:\n",
    "\n",
    "- **Kernel matrix**\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{K}=\\boldsymbol{\\Phi}_{X} \\boldsymbol{\\Phi}_{X}^{T}\n",
    "  $$\n",
    "\n",
    "  with \n",
    "\n",
    "  $$[\\boldsymbol{K}]_{i j}=\\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{i}\\right)^{T} \\boldsymbol{\\phi}(\\boldsymbol{x}_{j}) = \\langle \\boldsymbol{\\phi}(\\boldsymbol{x}_{i}), \\boldsymbol{\\phi}(\\boldsymbol{x}_{j}) \\rangle = k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)$$\n",
    "\n",
    "- **Kernel vector**\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{k}\\left(\\boldsymbol{x}^{*}\\right)=\\left[\\begin{array}{c}\n",
    "  k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}^{*}\\right) \\\\\n",
    "  \\vdots \\\\\n",
    "  k\\left(\\boldsymbol{x}_{N}, \\boldsymbol{x}^{*}\\right)\n",
    "  \\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "  \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{1}\\right)^{T} \\boldsymbol{\\phi}(\\boldsymbol{x}^{*}) \\\\\n",
    "  \\vdots \\\\\n",
    "  \\phi\\left(\\boldsymbol{x}_{N}\\right)^{T} \\boldsymbol{\\phi}(\\boldsymbol{x}^{*})\n",
    "  \\end{array}\\right]=\\boldsymbol{\\Phi}_{X} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}^{*}\\right)\n",
    "  $$\n",
    "\n",
    "### Kernel Ridge Regression\n",
    "\n",
    "Ridge Regression: (See also: [Polynomial Regression (Generalized linear regression models)](quiver-note-url/E1C1BD63-C259-41DE-8252-635696F048C0))\n",
    "\n",
    "- - Squared error function + L2 regularization\n",
    "\n",
    "- Linear feature space\n",
    "\n",
    "- <span style=\"color:red\">**Not directly applicable in infinite dimensional feature spaces**</span>\n",
    "\n",
    "- **Objective:**\n",
    "\n",
    "  $$\n",
    "  L_{\\text {ridge }}=\\underbrace{(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})^{T}(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})}_{\\text {sum of squared errors }}+\\lambda \\underbrace{\\boldsymbol{w}^{T} \\boldsymbol{w}}_{L_{2} \\text { regularization }}\n",
    "  $$\n",
    "\n",
    "  - $\\boldsymbol{\\Phi}_{X}=\\left[\\begin{array}{c}\n",
    "    \\phi\\left(\\boldsymbol{x}_{1}\\right)^{T} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\phi\\left(\\boldsymbol{x}_{N}\\right)^{T}\n",
    "    \\end{array}\\right] \\in \\mathbb{R}^{N \\times d}$\n",
    "\n",
    "- Solution\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{w}_{\\text {ridge }}^{*}= \\color{red}{\\underbrace{\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\boldsymbol{I}\\right)^{-1}}_{d \\times d \\text { matrix inversion }}} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y}\n",
    "  $$\n",
    "\n",
    "  <span style=\"color:red\">Matrix inversion **infeasible** in **infinite** dimensions!!!üò≠</span>\n",
    "\n",
    "#### Apply kernel trick\n",
    "\n",
    "Rewrite solution as **inner products** of the feature space with the following matrix identity\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{I} + \\boldsymbol{A}\\boldsymbol{B})^{-1}\\boldsymbol{A} = \\boldsymbol{A} (\\boldsymbol{I} + \\boldsymbol{B}\\boldsymbol{A})^{-1}\n",
    "$$\n",
    "\n",
    "Then we get\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\boldsymbol{w}_{\\text {ridge }}^{*} \n",
    "&= \\color{red}{\\underbrace{\\left(\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}+\\lambda \\boldsymbol{I}\\right)^{-1}}_{d \\times d \\text { matrix inversion }}} \\boldsymbol{\\Phi}^{T} \\boldsymbol{y} \\\\\n",
    "& \\overset{}{=} \\boldsymbol{\\Phi}^{T} \\color{LimeGreen}{\\underbrace{\\left( \\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^{T} +\\lambda \\boldsymbol{I}\\right)^{-1}}_{N \\times N \\text { matrix inversion }}} \\boldsymbol{y} \\\\\n",
    "&= \\boldsymbol{\\Phi}^{T} \\underbrace{\\left( \\boldsymbol{K} +\\lambda \\boldsymbol{I}\\right)^{-1}\\boldsymbol{y}}_{=: \\boldsymbol{\\alpha}} \\\\\n",
    "&= \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\alpha} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- beneficial for $d \\gg N$\n",
    "- **Still, $\\boldsymbol{w}^* \\in \\mathbb{R}^d$ is potentially infinite dimensional and can not be represented**\n",
    "\n",
    "Yet, we can still evaluate the function $f$ without the explicit representation of $\\boldsymbol{w}^*$ üòâ\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "f(\\boldsymbol{x}) \n",
    "& =\\boldsymbol{\\phi}(\\boldsymbol{x})^{T} \\boldsymbol{w}^{*} \\\\\n",
    "& \\overset{\\eqref{eq: ridge reg w*}}{=}\\boldsymbol{\\phi}(\\boldsymbol{x})^{T} \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\alpha} \\\\\n",
    "& \\overset{\\text{kernel} \\\\ \\text{trick}}{=}\\boldsymbol{k}(\\boldsymbol{x})^{T} \\boldsymbol{\\alpha} \\\\\n",
    "& =\\sum_{i} \\alpha_{i} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For a **Gaussian kernel**\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x})=\\sum_{i} \\alpha_{i} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right)=\\sum_{i} \\alpha_{i} \\exp \\left(-\\frac{\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{i}\\right\\|^{2}}{2 \\sigma^{2}}\\right)\n",
    "$$\n",
    "\n",
    "#### Select hyperparameter\n",
    "\n",
    "Bandwidth parameter $\\sigma$ in Gaussian kernel \n",
    "\n",
    "$$\n",
    "k(\\boldsymbol{x}, \\boldsymbol{y})=\\exp \\left(-\\frac{\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^{2}}{2 \\sigma^{2}}\\right)\n",
    "$$\n",
    "\n",
    "are called **hyperparameters**.\n",
    "\n",
    "How to choose? **Cross validation!**\n",
    "\n",
    "Example:\n",
    "\n",
    "![image-20200305164457118](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/image-20200305164457118.png)\n",
    "\n",
    "### Summary: kernel ridge regression\n",
    "\n",
    "The solution for kernel ridge regression is given by\n",
    "$$\n",
    "f^{*}(\\boldsymbol{x})=\\boldsymbol{k}(\\boldsymbol{x})^{T}(\\boldsymbol{K}+\\lambda \\boldsymbol{I})^{-1} \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "- <span style=\"color:LimeGreen\">No evaluation of the feature vectors needed</span> üëè\n",
    "- <span style=\"color:LimeGreen\">Only pair-wise scalar products (evaluated by the kernel)</span> üëè\n",
    "- <span style=\"color:red\">Need to invert a </span> $\\color{red}{N \\times N}$ <span style=\"color:red\">matrix (can be costly)</span> ü§™\n",
    "\n",
    "‚ÄºÔ∏è**Note**:\n",
    "\n",
    "- Have to store **all samples** in kernel-based methods\n",
    "\n",
    "  - Computationally expensive (matrix inverse is $O(n^{2.376})$) !\n",
    "\n",
    "- Hyperparameters of the method are given by the kernel-parameters \n",
    "\n",
    "  - Can be optimized on **validation-set**\n",
    "\n",
    "- **Very flexible function representation, only few hyper-parameters** üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
