{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Machine Learning Project\n",
    "\n",
    "![e2e_ML_Project](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/e2e_ML_Project.png)\n",
    "\n",
    "## 1. Look at the big picture\n",
    "\n",
    "### 1.1 Frame the problem\n",
    "\n",
    "Consider the business objective: How do we expect to use and benefit from this model?\n",
    "\n",
    "### 1.2 Select a performance measure\n",
    "\n",
    "### 1.3 Check the assumptions\n",
    "\n",
    "List and verify the assumptions.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Get the data\n",
    "\n",
    "### 2.1 Download the data\n",
    "\n",
    "Automate this process: Create a small function to handle downloading, extracting, and storing data.\n",
    "\n",
    "### 2.2 Take a quick look at the data\n",
    "\n",
    "- Use `pandas.head()` to look at the top rows of the data\n",
    "- Use `pandas.info()` to get a quick description of the data\n",
    "  - For categorical attributes, use `value_counts()` to see categories and the #samples of each category\n",
    "  - For numerical attributes, use `describe()` to get a summary of the numerical attributes.\n",
    "\n",
    "### Create a test set\n",
    "\n",
    "- If dataset is large enough, use **purely random sampling**. (`train_test_split`)\n",
    "\n",
    "- If the test set need to be representative of the overall data, use **stratified sampling**.\n",
    "\n",
    "  \n",
    "\n",
    "## 3. Discover and visualize the data to gain insights\n",
    "\n",
    "1. Make sure put the test set aside and only explore the training set\n",
    "2. If the trainingset is very large, sample an exploration set to make manipulations easy and fast\n",
    "\n",
    "### 3.1 Visualizing data\n",
    "\n",
    "### 3.2 Look for correlations\n",
    "\n",
    "Two ways:\n",
    "\n",
    "- Compute the **standard correlation coefficient** (also called **Pearson's r**) between every pair of attributes using the `corr()` method.\n",
    "- Or use `panda`'s `scatter_matrix` function\n",
    "\n",
    "### 3.3 Experimenting with attribute combinations\n",
    "\n",
    "\n",
    "\n",
    "## 4. Prepare the data for ML algorithms\n",
    "\n",
    "**Firstly, ensure a clean training set and separate the predictors and labels.**\n",
    "\n",
    "### 4.1 Data cleaning\n",
    "\n",
    "Handle missing features: \n",
    "\n",
    "- Get rid of the corresponding samples (districts) -> use `dropna()`\n",
    "- Get rid of the whole attribute -> use `drop()`\n",
    "- Set the values to some value (zero, the mean, the median, etc.) -> use `fillna()`\n",
    "\n",
    "Or apply `SimpleImputer` from Scikit-Learn to all the numerical attributes.\n",
    "\n",
    "### 4.2 Handle text and categorical attributes\n",
    "\n",
    "Most ML algorithms prefer to work with numbers anyway.\n",
    "Transform text and categorical attributes to numerical attributes Using One-hot encoding.\n",
    "\n",
    "### 4.3 Custom transformers\n",
    "\n",
    "The custom transformer should work seamlessly with Scikit-Learn functionalities (such as pipelines).\n",
    "-> Create a class and implement three methods:\n",
    "\n",
    "- `fit()`\n",
    "- `transform()`\n",
    "- `fit_transform()` (can get it by simply adding `TransfromerMixin` as a base class)\n",
    "\n",
    "If we add `BaseEstimator` as a bass class, we can get two extra methods \n",
    "\n",
    "- `get_params()`\n",
    "- `set_params()`\n",
    "  that will be useful for automatic hyperparameter tuning.\n",
    "\n",
    "### 4.4 Feature scaling\n",
    "\n",
    "Comman ways:\n",
    "\n",
    "- **Min-max scaling (normalization)**: Use `MinMaxScalar`\n",
    "- **Standardization**\n",
    "  - Use `StandardScalar`\n",
    "  - Less affected by outliners\n",
    "\n",
    "### 4.5 Transformation pipelines\n",
    "\n",
    "Group sequences of transformations into one step.\n",
    "\n",
    "`Pipeline` from `scikit-learn`:\n",
    "\n",
    "- a list of name/estimator pairs defining a sequence of steps\n",
    "- the last estimator must be transformers (must have a `fit_transform()` method)\n",
    "- names can be anything but must be unique and don't contain double underscores \"__\"\n",
    "\n",
    "More convenient is to use a **single** transformer to handle the categorical columns and the numerical columns.\n",
    "-> Use `ColumbTransformer`: handle all columns, applying the appropriate transformations to each column and also works great with Pandas DataFrames.\n",
    "\n",
    "\n",
    "\n",
    "## 5. Select a model and train it\n",
    "\n",
    "### 5.1 Train and evaluate on the trainging set\n",
    "\n",
    "### 5.2 Better evaluation using Cross-Validation\n",
    "\n",
    "\n",
    "\n",
    "## 6. Fine-tune the model\n",
    "\n",
    "### 6.1 Grid search\n",
    "\n",
    "When exploring **relatively few** combinations, use `GridSearchCV`: Tell it which hyperparameters we want to experiment with, and what values to try out. Then it will evaluate all the possible combinations of hyperparameter values, using cross-validation.\n",
    "\n",
    "### 6.2 Randomized search\n",
    "\n",
    "When the hyperparameter search space is **large**, use `RandomizedSearchCV`. It evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\n",
    "\n",
    "### 6.3 Ensemble methods\n",
    "\n",
    "Try to combine the models that perform best. (More see: [ðŸ’¡ Why ensemble learning?](quiver-note-url/122498BB-8DC0-47F9-873E-26877F6452BA))\n",
    "\n",
    "### 6.4 Analyze the best models and their errors\n",
    "\n",
    "Gain good insights on the problem by inspecting the best models.\n",
    "\n",
    "### 6.5 Evaluate the system on the test set\n",
    "\n",
    "1. Get the predictors and labels from test set\n",
    "\n",
    "2. Run full pipeline to transform the data \n",
    "\n",
    "3. Evaluate the final model on the test set\n",
    "\n",
    "    \n",
    "\n",
    "## 7. Present the solution\n",
    "\n",
    "\n",
    "\n",
    "## 8. Launch, monitor, and maintain the system\n",
    "\n",
    "- Plug the production input data source into the system and write test\n",
    "- Write monitoring code to check system's live performance at regular intervals and trigger alerts when it drops\n",
    "- Evaluate the system's input data quality\n",
    "- Train the models on a regular basis using fresh data (automate this precess as much as possible!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
