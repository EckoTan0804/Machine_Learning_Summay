{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "üí° **Use regression algorithm for classification**\n",
    "\n",
    "Logistic regression: **estimate the probability that an instance belongs to a particular class** \n",
    "\n",
    "- If the estimated probability is **greater than 50%**, then the model predicts that the instance belongs to that class (called the **positive** class, labeled ‚Äú1‚Äù), \n",
    "- or else it predicts that it does not (i.e., it belongs to the **negative** class, labeled ‚Äú0‚Äù). \n",
    "\n",
    "This makes it a **binary** classifier. \n",
    "\n",
    "## Logistic / Sigmoid function\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Sigmoid-function-2.svg\" style=\"zoom:60%; background-color:white\">\n",
    "\n",
    "$\\sigma(t)=\\frac{1}{1+\\exp (-t)}$\n",
    "\n",
    "- Bounded: $\\sigma(t) \\in (0, 1)$\n",
    "\n",
    "- Symmetric: $1 - \\sigma(t) = \\sigma(-t)$\n",
    "\n",
    "- Derivative: $\\sigma^{\\prime}(t)=\\sigma(t)(1-\\sigma(t))$\n",
    "\n",
    "  \n",
    "\n",
    "## Estimating probabilities and making prediction\n",
    "\n",
    "1. Computes a weighted sum of the input features (plus a bias term) \n",
    "\n",
    "2. Outputs the logistic of this result\n",
    "\n",
    "   $\\hat{p}=h_{\\theta}(\\mathbf{x})=\\sigma\\left(\\mathbf{x}^{\\mathrm{T}} \\boldsymbol{\\theta}\\right)$\n",
    "\n",
    "3. Prediction: \n",
    "\n",
    "   $\\hat{y}=\\left\\{\\begin{array}{ll}0 & \\hat{p}<0.5\\left(\\Leftrightarrow h_{\\theta}(\\mathbf{x})<0\\right) \\\\ 1 & \\hat{p} \\geq 0.5\\left(\\Leftrightarrow h_{\\theta}(\\mathbf{x}) \\geq 0\\right)\\end{array}\\right.$\n",
    "\n",
    "\n",
    "\n",
    "## Train and cost function\n",
    "\n",
    "Objective of training: to set the parameter vector $\\boldsymbol{\\theta}$ so that the model estimates:\n",
    "\n",
    "- high probabilities ($\\geq 0.5$) for positive instances ($y=1$)\n",
    "- low probabilities ($< 0.5$) for negative instances ($y=0$)\n",
    "\n",
    "### Cost function of a single training instance:\n",
    "\n",
    "$c(\\boldsymbol{\\theta})=\\left\\{\\begin{array}{cc}-\\log (\\hat{p}) & y=1 \\\\ -\\log (1-\\hat{p}) & y=0\\end{array}\\right.$\n",
    "\n",
    "> <img src=\"https://miro.medium.com/max/1621/1*_NeTem-yeZ8Pr9cVUoi_HA.png\" style=\"zoom:30%; background-color:white\">\n",
    ">\n",
    "> - Actual lable: $y=1$, Misclassification: $\\hat{y} = 0 \\Leftrightarrow$ $\\hat{p} = \\sigma(h_{\\boldsymbol{\\theta}}(x))$ close to 0 $\\Leftrightarrow c(\\boldsymbol{\\theta})$ large \n",
    "> - Actual lable: $y=0$, Misclassification: $\\hat{y} = 1 \\Leftrightarrow$ $\\hat{p} = \\sigma(h_{\\boldsymbol{\\theta}}(x))$ close to 1 $\\Leftrightarrow c(\\boldsymbol{\\theta})$ large \n",
    "\n",
    "### The cost function over the whole training set\n",
    "\n",
    "Simply the average cost over all training instances (Combining the expressions of two different cases above into one single expression):\n",
    "\n",
    "$\\begin{aligned} J(\\boldsymbol{\\theta}) &=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(\\hat{p}^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{p}^{(i)}\\right)\\right] \\\\ &=\\frac{1}{m} \\sum_{i=1}^{m}\\left[-y^{(i)} \\log \\left(\\hat{p}^{(i)}\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{p}^{(i)}\\right)\\right] \\end{aligned}$\n",
    "\n",
    "> - $y^{(i)} =1:-y^{(i)} \\log \\left(\\hat{p}^{(i)}\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{p}^{(i)}\\right)=-\\log \\left(\\hat{p}^{(i)}\\right)$\n",
    "> - $y^{(i)} =0:-y^{(i)} \\log \\left(\\hat{p}^{(i)}\\right)-\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{p}^{(i)}\\right)=-\\log \\left(1-\\hat{p}^{(i)}\\right)$\n",
    ">   (Exactly the same as $c(\\boldsymbol{\\theta})$ for a single instance above üëè)\n",
    "\n",
    "### Training \n",
    "\n",
    "- No closed-form equation ü§™\n",
    "\n",
    "- But it is convex so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum     \n",
    "\n",
    "- Partial derivatives of the cost function with regards to the $j$-th model parameter $\\theta_j$:\n",
    "\n",
    "  $\\frac{\\partial}{\\partial \\theta_{j}} J(\\boldsymbol{\\theta})=\\frac{1}{m} \\displaystyle \\sum_{i=1}^{m}\\left(\\sigma\\left(\\boldsymbol{\\theta}^{T} \\mathbf{x}^{(l)}\\right)-y^{(i)}\\right) x_{j}^{(i)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
