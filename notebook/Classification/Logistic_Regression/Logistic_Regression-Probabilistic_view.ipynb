{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Probabilistic view)\n",
    "\n",
    "Class label: \n",
    "\n",
    "$\n",
    "y_i \\in \\{0, 1\\}\n",
    "$\n",
    "\n",
    "Conditional probability distribution of the class label is\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "p(y=1|\\boldsymbol{x}) &= \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}+b) \\\\\n",
    "p(y=0|\\boldsymbol{x}) &= 1 - \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}+b)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "with \n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1+\\operatorname{exp}(-x)}$\n",
    "\n",
    "This is a **conditional Bernoulli distribution**. Therefore, the probability can be represented as\n",
    "\n",
    "$\n",
    "\\begin{array}{ll}\n",
    "p(y|\\boldsymbol{x}) &= p(y=1|\\boldsymbol{x})^y p(y=0|\\boldsymbol{x})^{1-y} \\\\\n",
    "& = \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}+b)^y (1 - \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}+b))^{1-y}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "The **conditional Bernoulli log-likelihood** is (assuming training data is i.i.d)\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\operatorname{loglik}(\\boldsymbol{w}, \\mathcal{D}) \n",
    "&= \\log(\\operatorname{lik}(\\boldsymbol{w}, \\mathcal{D})) \\\\\n",
    "&= \\log(\\displaystyle\\prod_i p(y_i|\\boldsymbol{x}_i)) \\\\\n",
    "&= \\log\\left(\\displaystyle\\prod_i \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)^y \\left(1 - \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)\\right)^{1-y}\\right) \\\\\n",
    "&= \\displaystyle\\sum_i y\\log\\left(\\sigma(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)\\right)+ (1-y)\\log\\left(1 - \\sigma(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)\\right) \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Let \n",
    "\n",
    "$\n",
    "\\tilde{\\boldsymbol{w}}=\\left(\\begin{array}{c}1 \\\\ \\boldsymbol{w} \\end{array}\\right), \\quad \\tilde{\\boldsymbol{x}_i}=\\left(\\begin{array}{c}b \\\\ \\boldsymbol{x}_i \\end{array}\\right)\n",
    "$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\n",
    "\\operatorname{loglik}(\\boldsymbol{w}, \\mathcal{D}) = \\operatorname{loglik}(\\tilde{\\boldsymbol{w}}, \\mathcal{D})  = \\displaystyle\\sum_i y\\log\\left(\\sigma(\\tilde{\\boldsymbol{w}}^T\\tilde{\\boldsymbol{x}_i})\\right)+ (1-y)\\log\\left(1 - \\sigma(\\tilde{\\boldsymbol{w}}^T\\tilde{\\boldsymbol{x}_i}))\\right)\n",
    "$\n",
    "\n",
    "Our objective is to find the $\\tilde{\\boldsymbol{w}}^*$ that **maximize the log-likelihood**, i.e.\n",
    "\n",
    "$\n",
    "\\begin{array}{cl}\n",
    "\\tilde{\\boldsymbol{w}}^* &= \\underset{\\tilde{\\boldsymbol{w}}}{\\arg \\max} \\quad \\operatorname{loglik}(\\tilde{\\boldsymbol{w}}, \\mathcal{D}) \\\\\n",
    "&= \\underset{\\tilde{\\boldsymbol{w}}}{\\arg \\min} \\quad -\\operatorname{loglik}(\\tilde{\\boldsymbol{w}}, \\mathcal{D})\\\\\n",
    "&= \\underset{\\tilde{\\boldsymbol{w}}}{\\arg \\min} \\quad \\underbrace{-\\left(\\displaystyle\\sum_i y\\log\\left(\\sigma(\\tilde{\\boldsymbol{w}}^T\\tilde{\\boldsymbol{x}_i})\\right) + (1-y)\\log\\left(1 - \\sigma(\\tilde{\\boldsymbol{w}}^T\\tilde{\\boldsymbol{x}_i}))\\right)\\right)}_{\\text{cross-entropy loss}}\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "In other words, **maximizing the (log-)likelihood is the same as minimizing the cross entropy.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
