{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "## Goal of SVM\n",
    "\n",
    "To find the optimal separating hyperplane which **maximizes the margin** of the training data\n",
    "\n",
    "- it **correctly** classifies the training data\n",
    "- it is the one which will generalize better with unseen data (as far as possible from data points from each category)\n",
    "\n",
    "\n",
    "\n",
    "## SVM math formulation\n",
    "\n",
    "Assuming data is linear separable\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/image-20200304135136513.png\" alt=\"image-20200304135136513\" style=\"zoom:50%;\" />\n",
    "\n",
    "- **Decision boundary**: Hyperplane $\\mathbf{w}^{T} \\mathbf{x}+b=0$\n",
    "\n",
    "- **Support Vectors:** Data points closes to the decision boundary (Other examples can be ignored)\n",
    "\n",
    "  - **Positive** support vectors: $\\mathbf{w}^{T} \\mathbf{x}_{+}+b=+1$\n",
    "  - **negative** support vectors: $\\mathbf{w}^{T} \\mathbf{x}_{-}+b=-1$\n",
    "\n",
    "  > Why do we use 1 and -1 as class labels?\n",
    "  >\n",
    "  > - This makes the math manageable, because -1 and 1 are only different by the sign. We can write a single equation to describe the margin or how close a data point is to our separating hyperplane and not have to worry if the data is in the -1 or +1 class.\n",
    "\n",
    "> - If a point is far away from the separating plane on the positive side, then wTx+b will be a large positive number, and $label*(w^Tx+b)$ will give us a large number. If it‚Äôs far from the negative side and has a negative label, $label*(w^Tx+b)$ will also give us a large positive number.\n",
    "\n",
    "- **Margin** $\\rho$ : distance between the support vectors and the decision boundary and should be **maximized**\n",
    "\n",
    "  - $\\rho = \\frac{\\mathbf{w}^{T} \\mathbf{x}_{+}+b}{\\|\\mathbf{w}\\|}-\\frac{\\mathbf{w}^{T} \\mathbf{x}_{-}+b}{\\|\\mathbf{w}\\|}=\\frac{2}{\\|\\mathbf{w}\\|}$\n",
    "\n",
    "### SVM optimization problem\n",
    "\n",
    "Requirement:\n",
    "\n",
    "1. Maximal margin\n",
    "2. Correct classification\n",
    "\n",
    "Based on these requirements, we have:\n",
    "\n",
    "$\\begin{array}{lll} \\underset{\\mathbf{w}}{\\operatorname{argmax}} \\quad & \\frac{2}{\\|\\mathbf{w}\\|} \\qquad \\qquad &(\\text{Maximize margin})\\\\ \\text { s.t. } \\quad & \\mathbf{w}^{T} \\mathbf{x}_{i}+b\\left\\{\\begin{array}{ll}\\geq+1, & \\text { falls } y_{i}=+1 \\\\ \\leq-1, & \\text { falls } y_{i}=-1\\end{array} \\right.\\qquad \\qquad &(\\text{Condition for margin})\n",
    "\\end{array}$\n",
    "\n",
    "Reformulation:\n",
    "\n",
    "$\n",
    "\\begin{aligned} \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\quad &\\|\\mathbf{w}\\|^{2} \\\\ \\text { s.t. } \\quad & y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1 \\end{aligned}\n",
    "$\n",
    "\n",
    "This is the **hard margin SVM**.\n",
    "\n",
    "\n",
    "\n",
    "### Soft margin SVM\n",
    "\n",
    "#### üí° Idea\n",
    "\n",
    "**\"Allow the classifier to make some mistakes\"** (Soft margin)\n",
    "\n",
    "‚û°Ô∏è **Trade-off between margin and classification accuracy** \n",
    "\n",
    "<img src=\"/Users/EckoTan/Dropbox/KIT/Master/Sem2/Maschinelles_Lernen/Zusammenfassung/markdown/L05-SVMs.assets/image-20200304141838595.png\" alt=\"image-20200304141838595\" style=\"zoom:50%;\" />\n",
    "\n",
    "- Slack-variables: ${\\color {blue}{\\xi_{i}}} \\geq 0$ \n",
    "\n",
    "- üí°**Allows violating the margin conditions**\n",
    "  $$\n",
    "  y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1- \\color{blue}{\\xi_{i}}\n",
    "  $$\n",
    "\n",
    "  - $0 \\leq \\xi_{i} \\leq 1$ : sample is between margin and decision boundary (<span style=\"color:red\">**margin violation**</span>)\n",
    "  - $\\xi_{i} \\geq 1$ : sample is on the wrong side of the decision boundary (<span style=\"color:red\">**misclassified**</span>)\n",
    "\n",
    "#### Soft Max-Margin\n",
    "\n",
    "Optimization problem\n",
    "$$\n",
    "\\begin{array}{lll} \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\quad &\\|\\mathbf{w}\\|^{2} + \\color{blue}{C \\sum_i^N \\xi_i} \\qquad \\qquad & \\text{(Punish large slack variables)}\\\\\n",
    "\\text { s.t. } \\quad & y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1 -\\color{blue}{\\xi_i}, \\quad \\xi_i \\geq 0 \\qquad \\qquad & \\text{(Condition for soft-margin)}\\end{array}\n",
    "$$\n",
    "\n",
    "- $C$ : regularization parameter, determines how important $\\xi$ should be\n",
    "  - **Small** $C$: Constraints have **little** influence ‚û°Ô∏è **large** margin\n",
    "  - **Large** $C$: Constraints have **large** influence ‚û°Ô∏è **small** margin\n",
    "  - $C$ infinite: Constraints are enforced ‚û°Ô∏è **hard** margin\n",
    "\n",
    "#### Soft SVM Optimization\n",
    "\n",
    "Reformulate into an unconstrained optimization problem\n",
    "\n",
    "1. Rewrite constraints: $\\xi_{i} \\geq 1-y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right)=1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)$\n",
    "2. Together with $\\xi_{i} \\geq 0 \\Rightarrow \\xi_{i}=\\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)$\n",
    "\n",
    "**Unconstrained optimization** (over $\\mathbf{w}$):\n",
    "$$\n",
    "\\underset{{\\mathbf{w}}}{\\operatorname{argmin}} \\underbrace{\\|\\mathbf{w}\\|^{2}}_{\\text {regularization }}+C \\underbrace{\\sum_{i=1}^{N} \\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)}_{\\text {loss function }}\n",
    "\\label{eq:unconstrained soft svm}\n",
    "$$\n",
    "Points are in 3 categories:\n",
    "\n",
    "- $y_{i} f\\left(\\boldsymbol{x}_{i}\\right) > 1$ : Point **outside** margin, **no contribution** to loss\n",
    "\n",
    "- $y_{i} f\\left(\\boldsymbol{x}_{i}\\right) = 1$: Point is **on** the margin, **no contribution** to loss as **in hard margin**\n",
    "- $y_{i} f\\left(\\boldsymbol{x}_{i}\\right) < 1$: <span style=\"color:red\">**Point violates the margin, contributes to loss**</span>\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "SVMs uses \"hinge\" loss (approximation of 0-1 loss)\n",
    "\n",
    "> [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)\n",
    ">\n",
    "> For an intended output $t=\\pm 1$ and a classifier score $y$, the hinge loss of the prediction $y$ is defined as \n",
    "> $$\n",
    "> \\ell(y)=\\max (0,1-t \\cdot y)\n",
    "> $$\n",
    "> Note that $y$ should be the \"raw\" output of the classifier's decision function, not the predicted class label. For instance, in linear SVMs, $y = \\mathbf{w}\\cdot \\mathbf{x}+ b$, where $(\\mathbf{w},b)$ are the parameters of the hyperplane and $mathbf{x}$ is the input variable(s).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"/Users/EckoTan/Dropbox/KIT/Master/Sem2/Maschinelles_Lernen/Zusammenfassung/markdown/L05-SVMs.assets/image-20200304172146690.png\" alt=\"image-20200304172146690\" style=\"zoom:40%;\" />\n",
    "\n",
    "\n",
    "\n",
    "The loss function of SVM is **convex**:\n",
    "\n",
    "<img src=\"/Users/EckoTan/Dropbox/KIT/Master/Sem2/Maschinelles_Lernen/Zusammenfassung/markdown/L05-SVMs.assets/image-20200304172349088.png\" alt=\"image-20200304172349088\" style=\"zoom: 33%;\" />\n",
    "\n",
    "I.e.,\n",
    "\n",
    "- There is only **one** minimum\n",
    "- We can find it with gradient descent\n",
    "- **However:** Hinge loss is **not differentiable!** ü§™\n",
    "\n",
    "\n",
    "\n",
    "## Sub-gradients\n",
    "\n",
    "For convex function $f: \\mathbb{R}^d \\to \\mathbb{R}$ :\n",
    "$$\n",
    "f(\\boldsymbol{z}) \\geq f(\\boldsymbol{x})+\\nabla f(\\boldsymbol{x})^{T}(\\boldsymbol{z}-\\boldsymbol{x})\n",
    "$$\n",
    "(Linear approximation underestimates function)\n",
    "\n",
    "<img src=\"/Users/EckoTan/Dropbox/KIT/Master/Sem2/Maschinelles_Lernen/Zusammenfassung/markdown/L05-SVMs.assets/image-20200304172748278.png\" alt=\"image-20200304172748278\" style=\"zoom:33%;\" />\n",
    "\n",
    "A **subgradient** of a convex function $f$ at point $\\boldsymbol{x}$ is any $\\boldsymbol{g}$ such that\n",
    "$$\n",
    "f(\\boldsymbol{z}) \\geq f(\\boldsymbol{x})+\\nabla \\mathbf{g}^{T}(\\boldsymbol{z}-\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "- Always exists (even $f$ is not differentiable)\n",
    "- If $f$ is differentiable at $\\boldsymbol{x}$, then: $\\boldsymbol{g}=\\nabla f(\\boldsymbol{x})$\n",
    "\n",
    "### Example\n",
    "\n",
    "$f(x)=|x|$\n",
    "\n",
    "- $x \\neq 0$ : unique sub-gradient is $g= \\operatorname{sign}(x)$\n",
    "- $x =0$ : $g \\in [-1, 1]$\n",
    "\n",
    "![img](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/220px-Absolute_value.svg.png)\n",
    "\n",
    "### Sub-gradient Method\n",
    "\n",
    "**Sub-gradient Descent**\n",
    "\n",
    "1. Given **convex** $f$, not necessarily differentiable\n",
    "2. Initialize $\\boldsymbol{x}_0$\n",
    "3. Repeat: $\\boldsymbol{x}_{t+1}=\\boldsymbol{x}_{t}+\\eta \\boldsymbol{g}$, where $\\boldsymbol{g}$ is any sub-gradient of $f$ at point $\\boldsymbol{x}_{t}$\n",
    "\n",
    "‚ÄºÔ∏è Notes: \n",
    "\n",
    "- Sub-gradients do not necessarily decrease $f$ at every step (no real descent method)\n",
    "- Need to keep track of the best iterate $\\boldsymbol{x}^*$\n",
    "\n",
    "#### Sub-gradients for hinge loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\left(\\mathbf{x}_{i}, y_{i} ; \\mathbf{w}\\right)=\\max \\left(0,1-y_{i} f\\left(\\mathbf{x}_{i}\\right)\\right) \\quad f\\left(\\mathbf{x}_{i}\\right)=\\mathbf{w}^{\\top} \\mathbf{x}_{i}+b\n",
    "$$\n",
    "\n",
    "<img src=\"/Users/EckoTan/Dropbox/KIT/Master/Sem2/Maschinelles_Lernen/Zusammenfassung/markdown/L05-SVMs.assets/image-20200304175930294.png\" alt=\"image-20200304175930294\" style=\"zoom:33%;\" />\n",
    "\n",
    "#### Sub-gradient descent for SVMs\n",
    "\n",
    "Recall the **Unconstrained optimization** for SVMs:\n",
    "$$\n",
    "\\underset{{\\mathbf{w}}}{\\operatorname{argmin}} \\quad C \\underbrace{\\sum_{i=1}^{N} \\max \\left(0,1-y_{i} f\\left(\\boldsymbol{x}_{i}\\right)\\right)}_{\\text {loss function }} + \\underbrace{\\|\\mathbf{w}\\|^{2}}_{\\text {regularization }}\n",
    "$$\n",
    "At each iteration, pick random training sample $(\\boldsymbol{x}_i, y_i)$\n",
    "\n",
    "- If $y_{i} f\\left(\\boldsymbol{x}_{i}\\right)<1$: ‚Äã\n",
    "  $$\n",
    "  \\boldsymbol{w}{t+1}=\\boldsymbol{w}{t}-\\eta\\left(2 \\boldsymbol{w}{t}-C y{i} \\boldsymbol{x}_{i}\\right)\n",
    "  $$\n",
    "\n",
    "- Otherwise: \n",
    "  $$\n",
    "  \\quad \\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta 2 \\boldsymbol{w}_{t}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "## Application of SVMs\n",
    "\n",
    "- Pedestrian Tracking\n",
    "- text (and hypertext) categorization\n",
    "- image classification\n",
    "- bioinformatics (Protein classification, cancer classification)\n",
    "- hand-written character recognition\n",
    "\n",
    "Yet, in the last 5-8 years, neural networks have outperformed SVMs on most applications.ü§™‚òπÔ∏èüò≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
