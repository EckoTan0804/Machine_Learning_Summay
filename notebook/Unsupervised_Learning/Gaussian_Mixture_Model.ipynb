{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "\n",
    "\n",
    "\n",
    "## Gaussian Distribution\n",
    "\n",
    "**Univariate**: The Probability Density Function (PDF) is:\n",
    "$$\n",
    "P(x | \\theta)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)\n",
    "$$\n",
    "\n",
    "- $\\mu$: mean\n",
    "- $\\sigma$: standard deviation\n",
    "\n",
    "![gaussian mixture models](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/gaussians.png)\n",
    "\n",
    "\n",
    "\n",
    "**Multivariate**: The Probability Density Function (PDF) is:\n",
    "$$\n",
    "P(x | \\theta)=\\frac{1}{(2 \\pi)^{\\frac{D}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)}{2}\\right)\n",
    "$$\n",
    "\n",
    "- $\\mu$: mean\n",
    "- $\\Sigma$: covariance\n",
    "- $D$: dimension of data\n",
    "\n",
    "![gaussian mixture models](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/gaussians-3d-300x224.png)\n",
    "\n",
    "\n",
    "\n",
    "### Learning\n",
    "\n",
    "For univariate Gaussian model, we can use Maximum Likelihood Estimation (MLE) to estimate parameter $\\theta$ :\n",
    "$$\n",
    "\\theta= \\underset{\\theta}{\\operatorname{argmax}} L(\\theta)\n",
    "$$\n",
    "Assuming data are i.i.d, we have:\n",
    "$$\n",
    "L(\\theta)=\\prod_{j=1}^{N} P\\left(x_{j} | \\theta\\right)\n",
    "$$\n",
    "For numerical stability, we usually use Maximum Log-Likelihood:\n",
    "$$\n",
    "\\begin{align}\t\\theta \t&= \\underset{\\theta}{\\operatorname{argmax}} L(\\theta) \\\\\t&= \\underset{\\theta}{\\operatorname{argmax}} \\log(L(\\theta)) \\\\\t&= \\underset{\\theta}{\\operatorname{argmax}} \\sum_{j=1}^{N} \\log P\\left(x_{j} | \\theta\\right)\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "## Gaussian Mixture Model\n",
    "\n",
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.\n",
    "\n",
    "![A Gaussian mixture of three normal distributions.](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/mYN2Q9VqZH-gaussian-mixture-example.png)\n",
    "\n",
    "Define:\n",
    "\n",
    "- $x_j$: the $j$-th observed data, $j=1, 2,\\dots, N$\n",
    "\n",
    "- $K$: number of Gaussian model components\n",
    "\n",
    "- $\\alpha_k$: probability that the observed data belongs to the $k$-th model component\n",
    "\n",
    "  - $\\alpha_k \\geq 0$\n",
    "  - $\\displaystyle \\sum_{k=1}^{K}\\alpha_k=1$\n",
    "\n",
    "- $\\phi(x|\\theta_k)$: probability density function of the $k$-th model component \n",
    "\n",
    "  - $\\theta_k = (\\mu_k, \\sigma_k^2)$\n",
    "\n",
    "- $\\gamma_{jk}$: probability that the $j$-th obeserved data belongs to the $k$-th model component\n",
    "\n",
    "- Probability density function of Gaussian mixture model:\n",
    "  $$\n",
    "  P(x | \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(x | \\theta_{k}\\right)\n",
    "  $$\n",
    "  For this model, parameter is $\\theta=\\left(\\tilde{\\mu}_{k}, \\tilde{\\sigma}_{k}, \\tilde{\\alpha}_{k}\\right)$.\n",
    "\n",
    "## Expectation-Maximum (EM)\n",
    "\n",
    "> *Expectation-Maximization (EM) is a statistical algorithm for finding the right model parameters. We typically use EM when the data has missing values, or in other words, when the data is incomplete.*\n",
    "\n",
    "These missing variables are called **latent variables**.\n",
    "\n",
    "- *NEVER* observed\n",
    "- We do *NOT* know the correct values in advance\n",
    "\n",
    "**Since we do not have the values for the latent variables, Expectation-Maximization tries to use the existing data to determine the optimum values for these variables and then finds the model parameters.** Based on these model parameters, we go back and update the values for the latent variable, and so on.\n",
    "\n",
    "The Expectation-Maximization algorithm has two steps:\n",
    "\n",
    "- **E-step:** In this step, the available data is used to estimate (guess) the values of the missing variables\n",
    "- **M-step:** Based on the estimated values generated in the E-step, the complete data is used to update the parameters\n",
    "\n",
    "### EM in Gaussian Mixture Model\n",
    "\n",
    "- Initialize the parameters ($K$ Gaussian distributionw with the mean $\\mu_1, \\mu_2,\\dots,\\mu_k$ and covariance $\\Sigma_1, \\Sigma_2, \\dots, \\Sigma_k$)\n",
    "\n",
    "- Repeat\n",
    "\n",
    "  - **E-step**: For each point $x_j$, calculate the probability that it belongs to cluster/distribution $k$\n",
    "\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  \\gamma_{j k} &= \\frac{\\text{Probability } x_j \\text{ belongs to cluster } k}{\\text{Sum of probability } x_j \\text{ belongs to cluster } 1, 2, \\dots, k} \\\\\n",
    "  &= \\frac{\\alpha_{k} \\phi\\left(x_{j} | \\theta_{k}\\right)}{\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(x_{j} | \\theta_{k}\\right)}\\qquad j=1,2, \\ldots, N ; k=1,2 \\ldots, K\n",
    "  \\end{align}\n",
    "  $$\n",
    "\n",
    "  â€‹\tThe value will be high when the point is assigned to the right cluster and lower otherwise\n",
    "\n",
    "  - **M-step**: update parameters\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{\\text{Number of points assigned to cluster } k}{\\text{Total number of points}} = \\frac{\\sum_{j=1}^{N} \\gamma_{j k}}{N} \\qquad k=1,2, \\ldots, K\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{k}=\\frac{\\sum_{j}^{N}\\left(\\gamma_{j k} x_{j}\\right)}{\\sum_{j}^{N} \\gamma_{j k}}\\qquad k=1,2, \\ldots, K\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{k}=\\frac{\\sum_{j}^{N} \\gamma_{j k}\\left(x_{j}-\\mu_{k}\\right)\\left(x_{j}-\\mu_{k}\\right)^{T}}{\\sum_{j}^{N} \\gamma_{j k}} \\qquad k=1,2, \\ldots, K\n",
    "$$\n",
    "\n",
    "until convergence ($\\left\\|\\theta_{i+1}-\\theta_{i}\\right\\|<\\varepsilon$)\n",
    "\n",
    "Visualization: \n",
    "\n",
    "![The EM algorithm updating the parameters of a two-component bivariate Gaussian mixture model.](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/ek1bu6ogj2-em_clustering_of_old_faithful_data.gif)\n",
    "\n",
    "## Reference\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/30483076\n",
    "- https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
