{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function\n",
    "\n",
    "Objective function:\n",
    "$$\n",
    "\\operatorname{Obj}(\\Theta)=\\underbrace{L(\\Theta)}_{\\text {Training Loss }}+\\underbrace{\\Omega(\\Theta)}_{\\text {Regularization }}\n",
    "$$\n",
    "\n",
    "- Training loss: measures how well the model fit on training data\n",
    "  $$\n",
    "  L=\\sum_{i=1}^{n} l\\left(y_{i}, g_{i}\\right)\n",
    "  $$\n",
    "\n",
    "  - Square loss: $l(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2$\n",
    "  - Logistic loss: $l(y_i, \\hat{y}_i) = y_i \\log(1 + e^{-\\hat{y}_i}) + (1 - y_i) \\log(1 + e^{\\hat{y}_i})$\n",
    "\n",
    "- Regularization: How complicated is the model?\n",
    "\n",
    "  - $L_2$ norm (Ridge): $\\omega(w) = \\lambda \\|w\\|^2$\n",
    "  - $L_1$ norm (Lasso): $\\omega(w) = \\lambda \\|w\\|$\n",
    "\n",
    "|                         | Objective Function                                           | Linear model? | Loss     | Regularization |\n",
    "| ----------------------- | ------------------------------------------------------------ | ------------- | -------- | -------------- |\n",
    "| **Ridge** regression    | $\\sum_{i=1}^{n}\\left(y_{i}-w^{\\top} x_{i}\\right)^{2}+\\lambda\\|w\\|^{2}$ | ✅             | square   | $L_2$          |\n",
    "| **Lasso** regression    | $\\sum_{i=1}^{n}\\left(y_{i}-w^{\\top} x_{i}\\right)^{2}+\\lambda\\|w\\|$ | ✅             | square   | $L_2$          |\n",
    "| **Logistic** regression | $\\sum_{i=1}^{n}\\left[y_{i} \\cdot \\ln \\left(1+e^{-w^{\\top} x_{i}}\\right)+\\left(1-y_{i}\\right) \\cdot \\ln \\left(1+e^{w^{\\top} x_{i}}\\right)\\right]+\\lambda\\|w\\|^{2}$ | ✅             | logistic | $L_1$          |\n",
    "\n",
    "\n",
    "\n",
    "## Why do we want to contain two component in the objective? \n",
    "\n",
    "- **Optimizing training loss encourages predictive models** \n",
    "\n",
    "  - *Fitting well in training data at least get you close to training data which is hopefully close to the underlying distribution* \n",
    "\n",
    "- **Optimizing regularization encourages simple models** \n",
    "\n",
    "  - *Simpler models tends to have smaller variance in future predictions, making prediction* *stable* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
