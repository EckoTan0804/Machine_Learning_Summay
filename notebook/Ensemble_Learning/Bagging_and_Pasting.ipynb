{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "- Bootstrap Aggregating (Boosting): Sampling **with** replacement\n",
    "\n",
    "  ![Boostrap_Aggregating](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Boostrap_Aggregating.png)\n",
    "\n",
    "- Pasting: Sampling **without** replacement\n",
    "\n",
    "\n",
    "\n",
    "## Explaination\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. \n",
    "\n",
    "One way to get a diverse set of classifiers: **use the same training algorithm for every predictor, but to train them on different random subsets of the training set**\n",
    "\n",
    "- Sampling  **with** replacement: **boostrap aggregating (Bagging)**\n",
    "- Sampling  **without** replacement: **pasting**\n",
    "\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the **statistical mode** \n",
    "\n",
    "- classification: the most frequent prediction (just like a hard voting classifier)\n",
    "- regression: average\n",
    "\n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. üëè\n",
    "\n",
    "Generally, the net result is that the ensemble has a **similar bias but a lower variance** than a single predictor trained on the original training set. \n",
    "\n",
    "\n",
    "##Advantages of Bagging and Pasting\n",
    "\n",
    "- Predictors can all be trained in parallel, via different CPU cores or even different servers. \n",
    "- Predictions can be made in parallel. \n",
    "\n",
    " -> They scale very well üëç \n",
    "\n",
    "## Bagging vs. Pasting\n",
    "\n",
    "- Bootstrapping     introduces a bit more diversity in the subsets that each predictor is     trained on, so bagging ends up with a **slightly** **higher bias** than pasting, but this also means that predictors end up being **less correlated** so the ensemble‚Äôs variance is reduced. \n",
    "\n",
    "- **Overall, bagging often results in better models**\n",
    "\n",
    "- However, if you have spare time and CPU power you can use cross- validation to evaluate both bagging and pasting and select the one that works best. \n",
    "\n",
    "## Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. This means that only about 63% of the training instances are sampled on average for each predictor.\n",
    "\n",
    "The remaining 37% of the training instances that are not sampled are called **out-of-bag (oob) instances.** Note that they are **not the same 37%** for all predictors. \n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
